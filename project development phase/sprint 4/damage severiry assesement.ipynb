{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfKi2blzHYsB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter \n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette(\"cubehelix\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dense, Dropout, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, History\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.regularizers import l2, l1"
      ],
      "metadata": {
        "id": "ZOlcmh83HdLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    model = VGG16(include_top=False, weights='imagenet')\n",
        "    \n",
        "    generator = datagen.flow_from_directory(train_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_train.npy', bottleneck_features_train)\n",
        "    \n",
        "    generator = datagen.flow_from_directory(validation_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_validation.npy', bottleneck_features_validation)\n"
      ],
      "metadata": {
        "id": "zmgXEsfjHfyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0]*(277) + [1]*(314) + [2]*(385))\n",
        "    train_labels = to_categorical(train_labels)\n",
        "    \n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(51) + [1]*(56) + [2]*(69))\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')\n",
        "    \n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "    \n",
        "    with open(location+'/top_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "    \n",
        "    return model, fit.history"
      ],
      "metadata": {
        "id": "KyZvObc_HjKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_categorical_model():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "    print(\"Model loaded.\")\n",
        "    \n",
        "    #input_tensor = Input(shape=(256,256,3))\n",
        "    #base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    \n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))\n",
        "    \n",
        "    top_model.load_weights(top_model_weights_path)\n",
        "    \n",
        "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
        "    \n",
        "    for layer in model.layers[:25]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.00001, momentum=0.9), metrics=['accuracy'])\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "    \n",
        "    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "    \n",
        "    fit = model.fit_generator(train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples//batch_size, verbose=1, callbacks=[checkpoint])\n",
        "    \n",
        "    with open(location+'/ft_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "        \n",
        "    return model, fit.history"
      ],
      "metadata": {
        "id": "iwTJPQOMHnRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_best_model_results(model_hist):\n",
        "    best_epoch = np.argmax(model_hist['val_acc'])\n",
        "    print('epoch:', best_epoch+1, ', val_acc:', model_hist['val_acc'][best_epoch], ', val_loss:', model_hist['val_loss'][best_epoch])"
      ],
      "metadata": {
        "id": "hYsuRE5_HrX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(hist, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    axes[0].plot(range(stop), hist['acc'], label='Training')\n",
        "    axes[0].plot(range(stop), hist['val_acc'], label='Validation')\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "    \n",
        "    axes[1].plot(range(stop), hist['loss'], label='Training')\n",
        "    axes[1].plot(range(stop), hist['val_loss'], label='Validation')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='upper right')\n",
        "    \n",
        "    plt.tight_layout();\n",
        "    \n",
        "    print(\"Best Model:\")\n",
        "    print_best_model_results(hist)\n"
      ],
      "metadata": {
        "id": "pKrdH1b-Hve-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_categorical_model(model, directory, labels):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    generator = datagen.flow_from_directory(directory, target_size=(img_height,img_width), batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
        "    \n",
        "    predictions = model.predict_generator(generator, len(labels))\n",
        "    \n",
        "    pred_labels = [0 if i<0.5 else 1 for i in predictions]\n",
        "    \n",
        "    print('')\n",
        "    print(classification_report(validation_labels, pred_labels))\n",
        "    print('')\n",
        "    cm = confusion_matrix(validation_labels, pred_labels)\n",
        "    return cm\n"
      ],
      "metadata": {
        "id": "nQl75qHAHx93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Dataset Details"
      ],
      "metadata": {
        "id": "gREiZAq-H1EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'data4'\n",
        "top_model_weights_path = location+'/top_model_weights.h5'\n",
        "fine_tuned_model_path = location+'/ft_model.h5'\n",
        "\n",
        "train_data_dir = location+'/training'\n",
        "validation_data_dir = location+'/validation'\n",
        "train_samples = [len(os.listdir(train_data_dir+'/'+i)) for i in sorted(os.listdir(train_data_dir))]\n",
        "nb_train_samples = 976\n",
        "validation_samples = [len(os.listdir(validation_data_dir+'/'+i)) for i in sorted(os.listdir(validation_data_dir))]\n",
        "nb_validation_samples = 176\n",
        "\n",
        "img_width, img_height = 256,256\n",
        "epochs = 50\n",
        "batch_size = 16\n"
      ],
      "metadata": {
        "id": "-xbYNLlPH4GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_bottleneck_features()"
      ],
      "metadata": {
        "id": "3AxQBsD4H7Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d4_model, d4_history = train_categorical_model()"
      ],
      "metadata": {
        "id": "MaLTM_FeH9IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(d4_history)"
      ],
      "metadata": {
        "id": "keP55MFgH_ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model, ft_history = finetune_categorical_model()\n"
      ],
      "metadata": {
        "id": "la0WeBFLICzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = load_model('data4/ft_model.h5')"
      ],
      "metadata": {
        "id": "38urZGv5IFT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels = np.array([0] * validation_samples[0] + \n",
        "                             [1] * validation_samples[1] +\n",
        "                             [2] * validation_samples[2])"
      ],
      "metadata": {
        "id": "D6_eUJ2EIJZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipe32(image_path, model):\n",
        "    urllib.request.urlretrieve(image_path, 'save.jpg')\n",
        "    img = load_img('save.jpg', target_size=(256,256))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,)+x.shape)/255\n",
        "    pred = model.predict(x)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    d = {0:'minor', 1:'moderate', 2:'severe'}\n",
        "    for key in d.keys():\n",
        "        if pred_labels[0] == key:\n",
        "            print(\"Validating severity of damage....Result:\",d[key])\n",
        "    print(\"Severity assessment complete.\")"
      ],
      "metadata": {
        "id": "uEcUI8giILrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('https://www.copartdirect.com/content/2007-kia-rio-front-end-damage.jpg')"
      ],
      "metadata": {
        "id": "DBS1bgeiIN5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe32('https://www.copartdirect.com/content/2007-kia-rio-front-end-damage.jpg', ft_model)\n"
      ],
      "metadata": {
        "id": "L9JLFt54IQbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('http://repairablecars-forsale.com/photos/Exotic_Wrecked_Cars_F430_Spider_Red_Ferrari.jpg')\n"
      ],
      "metadata": {
        "id": "InquZjaxIT-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe32('http://repairablecars-forsale.com/photos/Exotic_Wrecked_Cars_F430_Spider_Red_Ferrari.jpg', ft_model)"
      ],
      "metadata": {
        "id": "6zteUozjIWBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('https://di-uploads-pod1.dealerinspire.com/depaulachevy/uploads/2015/07/Scratch-2.jpg')\n"
      ],
      "metadata": {
        "id": "0b2ZC3b-IY5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fW_S_Bp7Icap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe32('https://di-uploads-pod1.dealerinspire.com/depaulachevy/uploads/2015/07/Scratch-2.jpg', ft_model)\n"
      ],
      "metadata": {
        "id": "JNTSUhs5IcmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}